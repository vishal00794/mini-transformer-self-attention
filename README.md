# mini-transformer-self-attention
This project shows why self-attention is the core mechanism behind LLMs. A tiny transformer is trained on CPU using open text data and compared against a baseline without attention. Under identical settings, single and multi-head self-attention dramatically improve context, coherence, long-range dependency learning, proving "attention" is the key.
