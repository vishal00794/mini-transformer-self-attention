# mini-transformer-self-attention
# ğŸ§  Attention Is All You Need (Even on CPU)

This project demonstrates **why attentionâ€”not scaleâ€”is the key mechanism behind modern Large Language Models (LLMs)**. Instead of training a huge transformer, we build **small, CPU-trainable models** and isolate the *exact contribution* of attention.

We compare three architectures trained under **identical conditions**:

1. **No Attention** (BiLSTM baseline)
2. **Single-Head Attention** (BiLSTM + additive attention)
3. **Multi-Head Self-Attention (QKV)** (BiLSTM + Transformer-style attention)

The only difference between models is the **attention mechanism**. Same data, same parameters, same training budget.

---

## ğŸ¯ Goal

To answer one core question:

> *Why does attention make language models dramatically betterâ€”even when models are small and trained on CPU?*

This repo is designed to be:

* **Educational** (every step explained)
* **Reproducible** (runs on Colab / CPU)
* **Portfolio-ready** (clean comparisons + visual evidence)

---

## ğŸ“Š Dataset

* **IMDb Movie Reviews (25k samples)**
* Binary sentiment classification (positive / negative)
* Real English text (not toy data)

Preprocessing:

* Tokenization
* Top-10k vocabulary
* Padding/truncation to fixed length

---

## ğŸ§± Model Architectures

### 1ï¸âƒ£ Baseline: BiLSTM (No Attention)

**Pipeline**:

```
Text â†’ Embedding â†’ BiLSTM â†’ Max Pool â†’ Classifier
```

**Characteristics**:

* Reads text leftâ†’right and rightâ†’left
* Produces contextual word representations
* Treats all words equally during pooling

**Limitation**:

* Cannot explicitly focus on important words
* Long sequences dilute key signals

---

### 2ï¸âƒ£ Single-Head Attention (Additive Attention)

**Pipeline**:

```
Text â†’ Embedding â†’ BiLSTM â†’ Attention â†’ Context Vector â†’ Classifier
```

**What changes**:

* Each word receives a learned *importance score*
* Softmax converts scores into attention weights
* Sentence representation becomes a **weighted sum** of words

**What it learns**:

* Which words matter most *globally*
* Suppresses noise words ("the", "is", etc.)

**Limitation**:

* One global notion of importance
* No word-to-word interaction

---

### 3ï¸âƒ£ Multi-Head Self-Attention (QKV)

**Pipeline**:

```
Text â†’ Embedding â†’ BiLSTM â†’ Multi-Head Self-Attention â†’ Pooling â†’ Classifier
```

**Key upgrade**:

* Uses **Query (Q), Key (K), Value (V)** projections
* Each word attends to every other word
* Multiple heads learn different linguistic relations

**Internally**:

```
Q = H Â· W_Q
K = H Â· W_K
V = H Â· W_V
Attention = softmax(QKáµ€ / âˆšd) Â· V
```

**Why this matters**:

* Captures negation ("not good")
* Models long-range dependencies
* Parallel attention heads learn syntax, sentiment, emphasis

This is **true Transformer-style attention**.

---

## ğŸ” What We Compare

All models are trained with:

* Same dataset
* Same vocabulary
* Same embedding size
* Same optimizer & learning rate
* Same training steps

We compare:

* **Validation loss curves**
* **Test accuracy, F1, precision, recall, ROC-AUC**
* **Generated predictions vs ground truth**
* **Attention heatmaps (interpretability)**

---

## ğŸ“ˆ Key Results (Qualitative)

### No Attention

* Fluent but generic predictions
* Misses key sentiment cues
* Sensitive to sentence length

### Single Attention

* Clearly focuses on sentiment words
* Better stability and accuracy
* Still limited by global attention

### Multi-Head Attention (QKV)

* Best overall performance
* Understands negation and structure
* Produces most coherent and accurate outputs

**Conclusion**:

> Even small models show that attentionâ€”not scaleâ€”is the real breakthrough.

---

## ğŸ”¥ Why This Project Matters

* Proves core LLM ideas without GPUs
* Shows *mechanism*, not just metrics
* Bridges RNNs â†’ Transformers
* Ideal for interviews, blogs, and portfolios

---

## ğŸš€ How to Run

1. Open in **Google Colab (CPU is enough)**
2. Install dependencies (PyTorch)
3. Run notebook top-to-bottom
4. Observe loss curves, metrics, and attention heatmaps

---

## ğŸ§  Takeaway

> You donâ€™t need billions of parameters to understand why LLMs work.
> Attention aloneâ€”properly isolatedâ€”explains most of the magic.

---

â­ If you find this useful, consider starring the repo.

